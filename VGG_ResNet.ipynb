{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Pretrained models\n"
      ],
      "metadata": {
        "id": "33ji2OCmBLJT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keras"
      ],
      "metadata": {
        "id": "TlpAJaDkIYVY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "j2er8_cUBEOD"
      },
      "outputs": [],
      "source": [
        "from keras.applications import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "dir(keras.applications)"
      ],
      "metadata": {
        "id": "DmMDi-cHESWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_model = VGG16(weights='imagenet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWh6-8CnD2F3",
        "outputId": "499a78af-c816-4892-aee6-62ea850686ff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467096/553467096 [==============================] - 5s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_img=['1.jpg', '2.jpg']\n",
        "for i in range(len(path_img)):\n",
        "  my_img = load_img(path_img[i], target_size=[224, 224, 3])\n",
        "  my_img = img_to_array(my_img)\n",
        "  my_img = np.expand_dims(my_img, axis=0)\n",
        "  my_img = preprocess_input(my_img)\n",
        "  pred = vgg_model.predict(my_img)\n",
        "  top_acc = 5\n",
        "  for j in range(top_acc):\n",
        "    print(decode_predictions(pred, top=top_acc)[0][j][1])\n",
        "  print('--------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Jy0S_T4E3M2",
        "outputId": "81eae902-737f-4b21-c6f6-5c6697728f63"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n",
            "bulbul\n",
            "hummingbird\n",
            "junco\n",
            "jay\n",
            "magpie\n",
            "--------------------\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "jay\n",
            "partridge\n",
            "ruffed_grouse\n",
            "quail\n",
            "ruddy_turnstone\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch"
      ],
      "metadata": {
        "id": "zT6nqi6hIViM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.models import vgg16\n",
        "from torchvision import transforms\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "m1FenAh8F377"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "dir(torchvision.models)"
      ],
      "metadata": {
        "id": "h1NtKINFJ8e9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch_vgg_model = vgg16(pretrained=True)\n",
        "# feedforward\n",
        "torch_vgg_model.eval()"
      ],
      "metadata": {
        "id": "J2HoqvL3JxD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess = transforms.Compose([transforms.Resize(256),\n",
        "                    transforms.CenterCrop(224),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                         std=[0.229, 0.224, 0.225])\n",
        "                    ])\n",
        "img = Image.open(path_img[1])\n",
        "img = preprocess(img)\n",
        "batch_img = torch.unsqueeze(img, 0)\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  pred = torch_vgg_model(batch_img)\n",
        "\n",
        "\n",
        "_, indices = torch.sort(pred, descending=True)\n",
        "print(indices[0][:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1Zsl2vtKpc0",
        "outputId": "43edc5ad-9faf-4ba0-8be7-7bee2516ef8c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 92, 133, 139,  86, 138])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"imagenet_class_name.txt\") as f:\n",
        "  classes = [line.strip() for line in f.readlines()]"
      ],
      "metadata": {
        "id": "AYiRAC6zNdwo"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, index = torch.max(pred, 1)\n",
        "percentage = torch.nn.functional.softmax(pred, dim=1)[0] * 100\n",
        "print(classes[index[0]], percentage[index[0]].item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efU31MrBSl8L",
        "outputId": "f98e90f7-ae7e-4801-d22e-dfbf11621556"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "91,coucal 26.01412582397461\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet"
      ],
      "metadata": {
        "id": "7eHqKqGlYS8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import resnet50, ResNet50_Weights"
      ],
      "metadata": {
        "id": "JgvwVLOISutY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Old weights with accuracy 76.130%\n",
        "RN50v1 = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# New weights with accuracy 80.858%\n",
        "RN50v2 = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
        "# Strings are also supported\n",
        "# resnet50(weights=\"IMAGENET1K_V2\")\n",
        "\n",
        "# Best available weights (currently alias for IMAGENET1K_V2)\n",
        "# Note that these weights may change across versions\n",
        "RN50vd = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
        "\n",
        "# No weights - random initialization\n",
        "RN50None = resnet50(weights=None)\n",
        "\n"
      ],
      "metadata": {
        "id": "HazjcSKYafQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from torchvision.models.quantization.inception import Inception_V3_QuantizedWeights\n",
        "from torchvision.io import read_image\n",
        "from torchvision.models.quantization import resnet50, ResNet50_QuantizedWeights\n",
        "\n",
        "img = read_image(path_img[1])\n",
        "\n",
        "# Step 1: Initialize model with the best available weights\n",
        "weights = ResNet50_QuantizedWeights.DEFAULT\n",
        "model = resnet50(weights=weights, quantize=True)\n",
        "# feed-forward\n",
        "model.eval()\n",
        "\n",
        "# Step 2: Initialize the inference transforms\n",
        "preprocess = weights.transforms()\n",
        "\n",
        "# Step 3: Apply inference preprocessing transforms\n",
        "batch = preprocess(img).unsqueeze(0)\n",
        "\n",
        "# Step 4: Use the model and print the predicted category\n",
        "prediction = model(batch).squeeze(0).softmax(0)\n",
        "class_id = prediction.argmax().item()\n",
        "score = prediction[class_id].item()\n",
        "category_name = weights.meta[\"categories\"][class_id]\n",
        "print(f\"{category_name}: {100 * score}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmdPrzilaw3j",
        "outputId": "dff61cfb-c4d4-4522-e3a5-bb121dc03e59"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jay: 25.964227318763733%\n"
          ]
        }
      ]
    }
  ]
}